{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywren, json, requests, time, bs4\n",
    "import pandas as pd\n",
    "from re import findall\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('expert_info_v1.csv')\n",
    "data = data[['expert_id', 'name', 'institution', 'href']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## American Enterprise Institue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_work(url):\n",
    "    r = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(r.text)\n",
    "    raw = soup.find('div', {'class': 'search-facet periodical-name'})\n",
    "    if not raw: return None, None\n",
    "    else: raw = raw.findAll('li')\n",
    "    media_appearance = [(i['data-facet-count'], i['data-facet-query'][15:]) for i in raw]\n",
    "    \n",
    "    works = []\n",
    "    i = 1\n",
    "    while True:\n",
    "        new_url = url + f'&wpsolr_page={i}'\n",
    "        r = requests.get(new_url)\n",
    "        soup = bs4.BeautifulSoup(r.text)\n",
    "        raw = soup.findAll('a', {'class': 'news-thumbnail'})\n",
    "        time = [i.text for i in soup.findAll('span', {'class': 'primary-18'})]\n",
    "        if len(time) == 0: break\n",
    "        i += 1\n",
    "        works.extend(list(zip(time, [i['href'] for i in raw])))\n",
    "        \n",
    "    return media_appearance, works\n",
    "\n",
    "def scrape_expert_aei(user_id):\n",
    "    expert = dict()\n",
    "    href = f'https://www.aei.org/profile/{user_id}/'\n",
    "    r = requests.get(href)\n",
    "    soup = bs4.BeautifulSoup(r.text)\n",
    "    \n",
    "    expert['title'] = soup.find('h5').text\n",
    "    raw = soup.find('div', {'class': 'minimize js-minimize wysiwyg'})\n",
    "    if not raw: texts = []\n",
    "    else: texts = [i.text for i in raw.findAll(['p', 'h2', 'li'])]\n",
    "    \n",
    "    try: \n",
    "        expert['bio'] = '\\n'.join(texts[:texts.index('Experience')]).strip()\n",
    "        expert['experience'] = '\\n'.join(texts[texts.index('Experience')+1: \n",
    "                             texts.index('Education')]).strip()\n",
    "        expert['education'] = '\\n'.join(texts[texts.index('Education')+1:]).strip()\n",
    "    except ValueError:\n",
    "        expert['bio'] = '\\n'.join(texts).strip()\n",
    "        expert['experience'] = ''\n",
    "        expert['education'] = ''\n",
    "    \n",
    "    work_link = [i['href'] for i in soup.findAll('a', {'class': 'cta'}) \n",
    "                 if \"View all\" in i.text]\n",
    "    assert('search-results' in work_link[0]); assert('type:event'!= work_link[0])\n",
    "    media_appearance, works = find_all_work(work_link[0])\n",
    "    expert['media_appearance'] = media_appearance\n",
    "    expert['works'] = works\n",
    "    \n",
    "    return {user_id: expert}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\r"
     ]
    }
   ],
   "source": [
    "#expert_aei = dict()\n",
    "for num, eid in enumerate(data[data['institution']=='aei']['expert_id']):\n",
    "    if num < 75: continue\n",
    "    print(num, end = '\\r')\n",
    "    expert_aei.update(scrape_expert_aei(eid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(expert_aei, open('expert_aei.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brookings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_users_brookings(user_id):\n",
    "    r = requests.get(f'https://www.brookings.edu/experts/{user_id}/')\n",
    "    expert = dict()\n",
    "    soup = bs4.BeautifulSoup(r.text)\n",
    "    try: expert['title'] = soup.find('h3', {'class': 'title'}).text\n",
    "    except AttributeError: expert['title'] = None\n",
    "    expert['bio'] = soup.find('div', \n",
    "                              {'class': 'expert-intro-text post-body'}).text.strip()\n",
    "    raw = soup.findAll(['dt', 'dd'])\n",
    "    info, key = dict(), ''\n",
    "    for i in raw:\n",
    "        if i.name == 'dt':\n",
    "            key = i.text.strip(); info[key] = []\n",
    "        else: info[key].append(i.text.strip())\n",
    "    expert.update(info)\n",
    "    expert['articles'] = find_articles(user_id)\n",
    "    return {user_id: expert}\n",
    "\n",
    "def find_articles(user_id):\n",
    "    articles = []\n",
    "    i = 0\n",
    "    while True:\n",
    "        url = f'https://www.brookings.edu/author/{user_id}/?type=all&paged={i}'\n",
    "        r = requests.get(url)\n",
    "        soup = bs4.BeautifulSoup(r.text)\n",
    "        if 'Page not found' in soup.findAll('title')[0].text: return articles\n",
    "        raw = list(zip(soup.findAll('h4', {'class': 'title'}), soup.findAll('time')))\n",
    "        new_added = [(i[0].find('a')['href'], i[1].text.strip()) \n",
    "                    for i in raw if i[0].find('a')]\n",
    "        if len(new_added) == 0: break\n",
    "        articles.extend(new_added)\n",
    "        i += 1\n",
    "        print(len(articles))\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_accounts = list(set(data[data['institution']=='brookings']['expert_id']))\n",
    "pwex = pywren.default_executor(job_max_runtime = 500)\n",
    "futures = pwex.map(scrape_users_brookings, tar_accounts)\n",
    "print(len(futures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dones, not_dones = pywren.wait(futures, pywren.ANY_COMPLETED)\n",
    "print(len(dones), len(not_dones), end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_data = dict()\n",
    "for num, item in enumerate(dones):\n",
    "    try: expert_data.update(item.result())\n",
    "    except (IndexError, AttributeError): continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(expert_data, open('expert_brookings.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heritage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'https://www.heritage.org/'\n",
    "\n",
    "def scrape_expert_heritage(expert_id):\n",
    "    expert = dict()\n",
    "    r = requests.get(f'https://www.heritage.org/staff/{expert_id}')\n",
    "    soup = bs4.BeautifulSoup(r.text)\n",
    "    raw = soup.find('h2', {'class':'expert-bio-card__expert-title'})\n",
    "    if raw: expert['title'] = raw.text.strip()\n",
    "    else: expert['title'] = ''\n",
    "    raw = soup.find('div', {'class': 'expert-bio__read-more-container'})\n",
    "    if raw: expert['bio'] = raw.text.strip()\n",
    "    else: expert['bio'] = ''\n",
    "\n",
    "    articles = []\n",
    "    i = 0 \n",
    "    while True:\n",
    "        url = f'https://www.heritage.org/staff/{expert_id}?page={i}'\n",
    "        r = requests.get(url)\n",
    "        soup = bs4.BeautifulSoup(r.text)\n",
    "        hrefs = [base+i['href'] for i in soup.findAll('a', {'class': 'result-card__title'})]\n",
    "        time = [i.text[:-10].strip() for i in soup.findAll('p', {'class': 'result-card__date'})]\n",
    "        new_added = list(zip(hrefs, time))\n",
    "        if len(new_added) == 0: break\n",
    "        articles.extend(new_added)\n",
    "        print(i, end = '\\r')\n",
    "        i += 1\n",
    "    expert['articles'] = articles\n",
    "    \n",
    "    return {expert_id: expert}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "tar_accounts = list(set(data[data['institution']=='heritage']['expert_id']))\n",
    "pwex = pywren.default_executor(job_max_runtime = 500)\n",
    "futures = pwex.map(scrape_expert_heritage, tar_accounts)\n",
    "print(len(futures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_dones = [1]\n",
    "while len(not_dones) != 0: \n",
    "    dones, not_dones = pywren.wait(futures, pywren.ANY_COMPLETED)\n",
    "    print(len(dones), len(not_dones), end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_data = dict()\n",
    "for num, item in enumerate(dones):\n",
    "    expert_data.update(item.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(expert_data, open('expert_heritage.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_expert_cap(expert_id):\n",
    "    r = requests.get(f'https://www.americanprogress.org/person/{expert_id}/')\n",
    "    soup = bs4.BeautifulSoup(r.text)\n",
    "    \n",
    "    expert = dict()\n",
    "    try: expert['title'] = soup.find('div', {'class': 'col-md-12'}).text.strip()\n",
    "    except AttributeError: expert['title'] = ''\n",
    "    try: expert['bio'] = soup.find('div', {'class': 'bio-text'}).text.strip()\n",
    "    except AttributeError: expert['bio'] = ''\n",
    "    \n",
    "    raw = soup.find('table', {'class': 'display responsive'})\n",
    "    if raw: raw = raw.findAll('td')\n",
    "    else: \n",
    "        expert['articles'] = []\n",
    "        return {expert_id: expert}\n",
    "    raw = [i.find('a')['href'] if i.find('a') else i.text for i in raw]\n",
    "    articles = np.reshape(raw, (-1,3)).tolist()\n",
    "    expert['articles'] = articles\n",
    "    return {expert_id: expert}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_accounts = list(set(data[data['institution']=='cap']['expert_id']))\n",
    "pwex = pywren.default_executor(job_max_runtime = 500)\n",
    "futures = pwex.map(scrape_expert_cap, tar_accounts)\n",
    "print(len(futures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_dones = [1]\n",
    "while len(not_dones) != 0: \n",
    "    dones, not_dones = pywren.wait(futures, pywren.ANY_COMPLETED)\n",
    "    print(len(dones), len(not_dones), end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_data = dict()\n",
    "for num, item in enumerate(dones):\n",
    "    try: expert_data.update(item.result())\n",
    "    except: continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(expert_data, open('expert_cap.json', 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
